\section{Closed-loop RRT}
\label{sec:planner}

To make use of this information cost function to guide exploration, we consider a sampling-based planning approach that can evaluate the predicted information gain throughout the environment. In addition, we wish to use the occupancy grid that is being updated online (as described in Sect.~\ref{sec:occupancy_grid_mapping}) to guide the vehicle around obstacles in the environment. The rapidly-exploring random tree (RRT) algorithm is well suited to planning paths through these types of large environments, and is described below. 

The planner starts from the vehicle's current state and samples a point $x_s$ in the environment. Using the occupancy grid, we can reject samples that lie in cells with a sufficiently high probability of being occupied. If the sample is valid, we find the ``best'' node in the tree of paths (initially just the vehicle state), where ``best'' is determined by the cost function, and add a new edge to the tree connecting the sample point to the nearest node.
Then a new sample is drawn and the process repeats to grow a tree of path segments through the environment. This tree-growing process terminates after a specified time, making this an any-time approach, and the minimum cost path is returned. Specifically, we consider a variant of the anytime RRT algorithm known as Closed-loop RRT (CL-RRT)~\cite{Kuwata09_TCST}, in which the edges are generated by forward simulating the closed-loop vehicle dynamics toward the sample point. This approach is traditionally used to ensure dynamic feasibility and dense collision checking. However, the forward simulation also means we have full state information for the system at the end of each segment. This allows us to evaluate the predicted information gain \eqref{eq:csqmi} at that point and assign a corresponding cost to the candidate trajectory.

However, CL-RRT is traditionally a goal-directed planner. Since we are primarily interested in exploring the environment, we modify CL-RRT to function as an exploration-driven planner. We first define the sampling distribution to be a Gaussian centered about the root of the tree, with no bias toward any direction (unlike standard sampling-based planners that will sample the goal some small probability). We also do not include any distance information in the node cost, in order to favor pure exploration. Finally, since there is no goal to guide the selection of the best branch from the tree, we simply select the branch with the minimum cost endpoint in the entire tree. This enables the planner to compute paths that aim to maximize the predicted information gain while keeping the robot in the free space ($\mathcal{X}_\text{free}(t)$, defined by the current occupancy map). The planner continuously replans at a set rate and selects the best path if it has a lower cost than the previously selected path (or if the old path is new infeasible due to map updates). The resulting modified CL-RRT algorithm is presented in Alg.~\ref{alg:clrrt}

\begin{algorithm} [t]
\caption{CL-RRT}
\label{alg:clrrt}
\begin{algorithmic}[1]
\State $t \gets 0, x(t) \gets x_\text{initial}$
\State Initialize tree with node at $x_\text{initial}$
\While{explore == true}
\State $t_\text{start} \gets t$
\For{$t - t_\text{start} <$ planning time}
\State Sample point $x_s$ from the environment
\State Select min-cost node from $n$ nearest in tree
\State $k \gets 0$
\State $\hat{x}(t+k) \gets $ last state at n
\While{$\hat{x}(t+k) \in \mathcal{X}_\text{free}(t)$, $\hat{x}(t+k) \neq x_s$}
	\State Compute command to drive system to $x_s$
	\State Simulate system, get end state $\hat{x}(t+k+1)$
	\State Compute cost based on \eqref{eq:csqmi}
	\State $k \gets k+1$
\EndWhile
\State Store final $x_\text{hat}$ as new node in tree
\EndFor
\State Get min-cost path from tree
\State Re-evaluate old path cost on current map
\If {new path cost $<$ old path cost}
\State Set new path
\EndIf
\State $t \gets t$ + planning time
\EndWhile
\end{algorithmic}
\end{algorithm}
