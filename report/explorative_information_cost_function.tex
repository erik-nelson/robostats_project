\section{Explorative Information Cost Function}
\label{section:explorative_information_cost_function}

The goal of exploration is to find a dynamically feasible sequential set of motions chosen from a library of motion primitives, $\mc{X}$, over a time interval, $\tau := t+1 : t+T$, which enable the robot to explore its environment. We use the criteria that an explorative action is one which allows the robot to position itself in locations that generate observations which are informative to the robot's map. By this criteria, choosing the optimal exploration action will maximize an information metric over the robot's \textit{future map}. In this context, a \textit{future map} is an updated version of the robot's current map, which has used Eq.~\eqref{eq:logodds_update} to incorporate all measurements gathered while executing an action. An \textit{action} can be defined as a discrete sequence of states, $x_{\tau} = \left[x_{t+1},\dots,x_{t+T}\right]$. While executing an action, the robot will obtain a set of measurements $z_{\tau}(x_{\tau}) = \left[z_{t+1}(x_{t+1}),\dots,z_{t+T}(x_{t+T})\right]$ by sensing from the states $x_{\tau}$. $z_{\tau}(x_{\tau})$ is modeled as a random variable whose distribution is parameterized by a deterministic action, $x_{\tau}$, generated by a planner. Under this notation, an explorative planner must determine $x_{\tau}^{*}$: the action that visits locations which allow the robot to obtain the set of measurements which are most informative to the current map. When integrated with Eq.~\eqref{eq:logodds_update}, these measurements will maximize an information-theoretic cost function over the map. We choose to maximize Shannon Mutual Information (MI) rate between the current map, $m$, and the measurements $z_{\tau}$ gathered along $x_{\tau}$.
%
\begin{align}
  \begin{split}
    x_{\tau}^{*}
    &=
    \argmax_{x_{\tau} \in \mc{X}^{T}}
    \frac{\text{I}_{\text{MI}}
      \left[
        m
        ;
        z_{\tau}(x_{\tau})
      \right]
    }
    {R\left(x_{\tau}\right)}
    =
    \argmax_{x_{\tau} \in \mc{X}^{T}}
    \frac{
      \text{H}
      \left[
        m
      \right]
      -
      \text{H}
      \left[
        m
        \ \vert \
        z_{\tau}(x_{\tau})
      \right]
    }
    {R\left(x_{\tau}\right)}
  \end{split}
\end{align}
%
where $\text{I}_{\text{MI}}$ is the Shannon Mutual Information, and $R: \mc{X}^{T} \rightarrow \mbb{R}^{+}$ is a function which returns the time required to execute an action. In contrast to MI, MI rate is used so that actions with different execution times can be compared with a common metric.

TALK ABOUT WHY $\text{H}\left[m\right]$ IS EASY.

Several assumptions can be made to simplify the optimization of $x_{\tau}^{*}$. Due to cell independence in the occupancy grid formulation, the joint conditional entropy over the map can be expressed as a sum of individual cell conditional entropies. Additionally, let $\mc{C}$ to be the set of cells in the map that beams in $z_{\tau}$ pass through. Given the measurement model (described in Sect.~\ref{sec:measurement_model}), cells $c \notin \mc{C}$ are not updated by $z_{\tau}$ and therefore do not contribute to the map's conditional entropy. Using these assumptions, we may write the entropy of the map conditioned on $z_{\tau}$ as
%
\begin{align}
  \begin{split}
    \text{H}
    \left[
      m
      \ \vert \
      z_{\tau}
    \right]
    &=
    \sum_{c \in \mc{C}}
    \text{H}
    \left[
      c
      \ \vert \
      z_{\tau}
    \right]
    \\
    &=
    \sum_{c \in \mc{C}}
    \expect_{c, z_{\tau}}
    \left[
      -\log
      p
      \left(
      c
      \ \vert \
      z_{\tau}
      \right)
    \right]
    \\
    &=
    -
    \int_{z_{\tau}}
    p
    \left(
    z_{\tau}
    \right)
    \sum_{c \in \mc{C}}
    o
    \left(
    c
    \ \vert \
    z_{\tau}
    \right)
    d z_{\tau}
    \label{eq:conditional_entropy}
  \end{split}
\end{align}
%
where
\begin{align}
  \begin{split}
    o(c \ \vert \ z_{\tau})
    &=
    p(c \ \vert \ z_{\tau})
    \log
    p(c \ \vert \ z_{\tau})
    +
    \left(
    1
    -
    p(c \ \vert \ z_{\tau})
    \right)
    \log
    \left(
    1
    -
    p(c \ \vert \ z_{\tau})
    \right)
  \end{split}
\end{align}
%
Computing this term requires integration over the space of measurements, $z_{\tau}$, which is intractable for an online planner. Instead. Instead, we approximate Eq.~\eqref{eq:conditional_entropy} with $N$ Monte Carlo samples, $\{z_{\tau}^{i}\}_{i=1}^{N}$, drawn from the distribution $p(z_{\tau})$.
%
\begin{align}
  \begin{split}
    \text{H}
    \left[
      m
      \ \vert \
      z_{\tau}
    \right]
    &\approx
    -\frac{1}{N}
    \sum_{i=1}^{N}
    \sum_{c \in \mc{C}}
    o
    \left(
    c
    \ \vert \
    z_{\tau}^{i}
    \right)
    \\
  \end{split}
\end{align}
%
Sampling
\begin{align}
  \begin{split}
    x_{\tau}^{*}
    &=
    \argmax_{x_{\tau} \in \mc{X}^{T}}
    \frac{1}{R\left(x_{\tau}\right)}
    \left(
      \text{H}
      \left[
        m
      \right]
      +
      \sum_{i=1}^{N}
      \sum_{c \in \mc{C}}
      o
      \left(
      c
      \ \vert
      \ z_{\tau}^{i}(x_{\tau})
      \right)
      \right)
  \end{split}
\end{align}
%

